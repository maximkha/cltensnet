{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cptn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.tensor([[0.,0.],[1.,0.],[0.,1.],[1.,1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.builder = CTNHelper()\n",
    "        self.builder.add(\"in_node1\", (2,2),   [\"in1\", \"u1\"])\n",
    "        self.builder.add(\"in_node2\", (2,2), [\"in2\", \"u2\"])\n",
    "        \n",
    "        self.builder.add(\"mid_node1\", (2,2, 2,2), [\"u1\", \"u2\", \"o1\", \"o2\"])\n",
    "        \n",
    "        self.builder.add(\"out_node1\", (2,2), [\"o1\", \"o2\"])\n",
    "        \n",
    "        self.params = nn.ParameterDict(map(toparam, self.builder.to_dict().items()))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.permute(1, 0)\n",
    "        \n",
    "        x1, x2 = x1.view(-1, 1), x2.view(-1, 1)\n",
    "        \n",
    "        in_node1 = batch_eval_poly(legendrend_to_pbasis(self.params[\"in_node1\"]), x1, \"in1\")\n",
    "        in_node2 = batch_eval_poly(legendrend_to_pbasis(self.params[\"in_node2\"]), x2, \"in2\")\n",
    "        \n",
    "        inner_int = batch_poly_mul(in_node1.align_to(*self.builder.all_axes_with_batch), legendrend_to_pbasis(self.params[\"mid_node1\"]))\n",
    "        merged_mid_in1 = batch_defn_integral(inner_int, (-1, 1), \"u1\").align_to(*self.builder.all_axes_with_batch)\n",
    "                \n",
    "        merged_mid_in1_in2 = batch_defn_integral(batch_poly_mul(in_node2.align_to(*self.builder.all_axes_with_batch), merged_mid_in1), (-1, 1), \"u2\").align_to(*self.builder.all_axes_with_batch)\n",
    "        merged_mid_in1_in2_out = batch_defn_integral(batch_defn_integral(batch_poly_mul(merged_mid_in1_in2, legendrend_to_pbasis(self.params[\"out_node1\"])), (-1, 1), \"o1\"), (-1, 1), \"o2\")\n",
    "        \n",
    "        return keep_axes(merged_mid_in1_in2_out, [\"batch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_axes=['u1', 'o2', 'in1', 'in2', 'o1', 'u2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:1106: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/c10/core/TensorImpl.h:1761.)\n",
      "  return super(Tensor, self).refine_names(names)\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: x.numel(), model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.tensor([[0.,0.],[1.,0.],[0.,1.],[1.,1.]])\n",
    "\n",
    "# and\n",
    "# ys = torch.tensor([[0],[0],[0],[1]]).float()\n",
    "# nand\n",
    "# ys = torch.tensor([[1],[1],[1],[0]]).float()\n",
    "# xor\n",
    "# ys = torch.tensor([[0],[1],[1],[0]]).float()\n",
    "# or\n",
    "# ys = torch.tensor([[0],[1],[1],[1]]).float()\n",
    "# nor\n",
    "# ys = torch.tensor([[1],[0],[0],[0]]).float()\n",
    "# ~xor\n",
    "ys = torch.tensor([[1],[0],[0],[1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.6433, -1.9709, -2.0122,  1.4906], grad_fn=<SqueezeBackward1>,\n",
       "       names=('batch',))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: loss.item()=3.7902841567993164\n",
      "[1]: loss.item()=3.6702611446380615\n",
      "[2]: loss.item()=3.5532331466674805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [1, 3, 1, 1, 3, 1], strides() = [0, 1, 0, 0, 3, 0]\n",
      "param.sizes() = [1, 3, 1, 1, 3, 1], strides() = [0, 1, 0, 0, 3, 0] (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/functions/accumulate_grad.h:202.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]: loss.item()=3.4392051696777344\n",
      "[4]: loss.item()=3.3281733989715576\n",
      "[5]: loss.item()=3.22013258934021\n",
      "[6]: loss.item()=3.115056276321411\n",
      "[7]: loss.item()=3.0129220485687256\n",
      "[8]: loss.item()=2.9137015342712402\n",
      "[9]: loss.item()=2.817370653152466\n",
      "[10]: loss.item()=2.7238967418670654\n",
      "[11]: loss.item()=2.633244037628174\n",
      "[12]: loss.item()=2.545368194580078\n",
      "[13]: loss.item()=2.4602296352386475\n",
      "[14]: loss.item()=2.3777787685394287\n",
      "[15]: loss.item()=2.2979605197906494\n",
      "[16]: loss.item()=2.22072172164917\n",
      "[17]: loss.item()=2.1460039615631104\n",
      "[18]: loss.item()=2.0737457275390625\n",
      "[19]: loss.item()=2.0038809776306152\n",
      "[20]: loss.item()=1.9363491535186768\n",
      "[21]: loss.item()=1.8710784912109375\n",
      "[22]: loss.item()=1.8080034255981445\n",
      "[23]: loss.item()=1.7470598220825195\n",
      "[24]: loss.item()=1.6881740093231201\n",
      "[25]: loss.item()=1.6312793493270874\n",
      "[26]: loss.item()=1.576311707496643\n",
      "[27]: loss.item()=1.5232032537460327\n",
      "[28]: loss.item()=1.471889615058899\n",
      "[29]: loss.item()=1.422305703163147\n",
      "[30]: loss.item()=1.3743945360183716\n",
      "[31]: loss.item()=1.3280925750732422\n",
      "[32]: loss.item()=1.2833466529846191\n",
      "[33]: loss.item()=1.2400976419448853\n",
      "[34]: loss.item()=1.1982955932617188\n",
      "[35]: loss.item()=1.1578885316848755\n",
      "[36]: loss.item()=1.1188275814056396\n",
      "[37]: loss.item()=1.0810649394989014\n",
      "[38]: loss.item()=1.0445585250854492\n",
      "[39]: loss.item()=1.0092639923095703\n",
      "[40]: loss.item()=0.9751418828964233\n",
      "[41]: loss.item()=0.9421508312225342\n",
      "[42]: loss.item()=0.9102545976638794\n",
      "[43]: loss.item()=0.8794165849685669\n",
      "[44]: loss.item()=0.8496034741401672\n",
      "[45]: loss.item()=0.8207799792289734\n",
      "[46]: loss.item()=0.7929141521453857\n",
      "[47]: loss.item()=0.7659755945205688\n",
      "[48]: loss.item()=0.7399353981018066\n",
      "[49]: loss.item()=0.7147634029388428\n",
      "[50]: loss.item()=0.6904321908950806\n",
      "[51]: loss.item()=0.6669149398803711\n",
      "[52]: loss.item()=0.644183874130249\n",
      "[53]: loss.item()=0.622215747833252\n",
      "[54]: loss.item()=0.6009846925735474\n",
      "[55]: loss.item()=0.5804680585861206\n",
      "[56]: loss.item()=0.5606408715248108\n",
      "[57]: loss.item()=0.5414828062057495\n",
      "[58]: loss.item()=0.5229705572128296\n",
      "[59]: loss.item()=0.5050841569900513\n",
      "[60]: loss.item()=0.4878016412258148\n",
      "[61]: loss.item()=0.471104234457016\n",
      "[62]: loss.item()=0.4549718499183655\n",
      "[63]: loss.item()=0.43938666582107544\n",
      "[64]: loss.item()=0.42432957887649536\n",
      "[65]: loss.item()=0.4097837507724762\n",
      "[66]: loss.item()=0.39573079347610474\n",
      "[67]: loss.item()=0.3821558356285095\n",
      "[68]: loss.item()=0.36904171109199524\n",
      "[69]: loss.item()=0.3563723862171173\n",
      "[70]: loss.item()=0.34413522481918335\n",
      "[71]: loss.item()=0.3323129415512085\n",
      "[72]: loss.item()=0.320892870426178\n",
      "[73]: loss.item()=0.309861421585083\n",
      "[74]: loss.item()=0.2992044985294342\n",
      "[75]: loss.item()=0.28891071677207947\n",
      "[76]: loss.item()=0.2789671719074249\n",
      "[77]: loss.item()=0.2693617641925812\n",
      "[78]: loss.item()=0.26008370518684387\n",
      "[79]: loss.item()=0.2511216700077057\n",
      "[80]: loss.item()=0.24246488511562347\n",
      "[81]: loss.item()=0.23410272598266602\n",
      "[82]: loss.item()=0.22602669894695282\n",
      "[83]: loss.item()=0.21822489798069\n",
      "[84]: loss.item()=0.2106902003288269\n",
      "[85]: loss.item()=0.2034127563238144\n",
      "[86]: loss.item()=0.19638444483280182\n",
      "[87]: loss.item()=0.18959587812423706\n",
      "[88]: loss.item()=0.18303966522216797\n",
      "[89]: loss.item()=0.17670850455760956\n",
      "[90]: loss.item()=0.1705937534570694\n",
      "[91]: loss.item()=0.1646890938282013\n",
      "[92]: loss.item()=0.1589871495962143\n",
      "[93]: loss.item()=0.153481587767601\n",
      "[94]: loss.item()=0.14816488325595856\n",
      "[95]: loss.item()=0.14303144812583923\n",
      "[96]: loss.item()=0.13807502388954163\n",
      "[97]: loss.item()=0.13328954577445984\n",
      "[98]: loss.item()=0.12866948544979095\n",
      "[99]: loss.item()=0.12420883774757385\n",
      "[100]: loss.item()=0.1199028342962265\n",
      "[101]: loss.item()=0.11574594676494598\n",
      "[102]: loss.item()=0.11173310875892639\n",
      "[103]: loss.item()=0.10785944014787674\n",
      "[104]: loss.item()=0.10412012785673141\n",
      "[105]: loss.item()=0.10051076114177704\n",
      "[106]: loss.item()=0.0970270037651062\n",
      "[107]: loss.item()=0.09366460889577866\n",
      "[108]: loss.item()=0.09041900187730789\n",
      "[109]: loss.item()=0.08728653192520142\n",
      "[110]: loss.item()=0.0842633992433548\n",
      "[111]: loss.item()=0.08134575188159943\n",
      "[112]: loss.item()=0.07853010296821594\n",
      "[113]: loss.item()=0.07581257820129395\n",
      "[114]: loss.item()=0.07318999618291855\n",
      "[115]: loss.item()=0.07065916806459427\n",
      "[116]: loss.item()=0.06821679323911667\n",
      "[117]: loss.item()=0.06586001813411713\n",
      "[118]: loss.item()=0.06358562409877777\n",
      "[119]: loss.item()=0.06139097735285759\n",
      "[120]: loss.item()=0.05927319824695587\n",
      "[121]: loss.item()=0.05722931772470474\n",
      "[122]: loss.item()=0.055257417261600494\n",
      "[123]: loss.item()=0.05335463955998421\n",
      "[124]: loss.item()=0.051518265157938004\n",
      "[125]: loss.item()=0.04974621161818504\n",
      "[126]: loss.item()=0.04803645238280296\n",
      "[127]: loss.item()=0.04638675972819328\n",
      "[128]: loss.item()=0.044794563204050064\n",
      "[129]: loss.item()=0.04325839504599571\n",
      "[130]: loss.item()=0.0417763888835907\n",
      "[131]: loss.item()=0.040345653891563416\n",
      "[132]: loss.item()=0.0389653742313385\n",
      "[133]: loss.item()=0.03763335198163986\n",
      "[134]: loss.item()=0.03634816035628319\n",
      "[135]: loss.item()=0.03510790690779686\n",
      "[136]: loss.item()=0.033911049365997314\n",
      "[137]: loss.item()=0.03275592252612114\n",
      "[138]: loss.item()=0.031641390174627304\n",
      "[139]: loss.item()=0.030565723776817322\n",
      "[140]: loss.item()=0.02952761948108673\n",
      "[141]: loss.item()=0.028525903820991516\n",
      "[142]: loss.item()=0.02755890227854252\n",
      "[143]: loss.item()=0.026625705882906914\n",
      "[144]: loss.item()=0.025725048035383224\n",
      "[145]: loss.item()=0.024855908006429672\n",
      "[146]: loss.item()=0.024016890674829483\n",
      "[147]: loss.item()=0.023207008838653564\n",
      "[148]: loss.item()=0.022425265982747078\n",
      "[149]: loss.item()=0.021670648828148842\n",
      "[150]: loss.item()=0.020942322909832\n",
      "[151]: loss.item()=0.02023928239941597\n",
      "[152]: loss.item()=0.019560283049941063\n",
      "[153]: loss.item()=0.01890496350824833\n",
      "[154]: loss.item()=0.01827242411673069\n",
      "[155]: loss.item()=0.017661383375525475\n",
      "[156]: loss.item()=0.01707165129482746\n",
      "[157]: loss.item()=0.016502048820257187\n",
      "[158]: loss.item()=0.015952300280332565\n",
      "[159]: loss.item()=0.015421167947351933\n",
      "[160]: loss.item()=0.014908324927091599\n",
      "[161]: loss.item()=0.014413030818104744\n",
      "[162]: loss.item()=0.013934752903878689\n",
      "[163]: loss.item()=0.013472776859998703\n",
      "[164]: loss.item()=0.013026618398725986\n",
      "[165]: loss.item()=0.01259548682719469\n",
      "[166]: loss.item()=0.012179126963019371\n",
      "[167]: loss.item()=0.011776967905461788\n",
      "[168]: loss.item()=0.01138834934681654\n",
      "[169]: loss.item()=0.011012955568730831\n",
      "[170]: loss.item()=0.010650168173015118\n",
      "[171]: loss.item()=0.01029979344457388\n",
      "[172]: loss.item()=0.009961085394024849\n",
      "[173]: loss.item()=0.009633852168917656\n",
      "[174]: loss.item()=0.009317493997514248\n",
      "[175]: loss.item()=0.009011962451040745\n",
      "[176]: loss.item()=0.008716563694179058\n",
      "[177]: loss.item()=0.008431056514382362\n",
      "[178]: loss.item()=0.008155032992362976\n",
      "[179]: loss.item()=0.007888242602348328\n",
      "[180]: loss.item()=0.007630339358001947\n",
      "[181]: loss.item()=0.007380995899438858\n",
      "[182]: loss.item()=0.007139934692531824\n",
      "[183]: loss.item()=0.006906833965331316\n",
      "[184]: loss.item()=0.006681483238935471\n",
      "[185]: loss.item()=0.006463578902184963\n",
      "[186]: loss.item()=0.006252954714000225\n",
      "[187]: loss.item()=0.006049091927707195\n",
      "[188]: loss.item()=0.005852025933563709\n",
      "[189]: loss.item()=0.005661410745233297\n",
      "[190]: loss.item()=0.0054770465940237045\n",
      "[191]: loss.item()=0.0052987658418715\n",
      "[192]: loss.item()=0.00512630445882678\n",
      "[193]: loss.item()=0.00495939701795578\n",
      "[194]: loss.item()=0.004797970876097679\n",
      "[195]: loss.item()=0.004641846753656864\n",
      "[196]: loss.item()=0.004490791354328394\n",
      "[197]: loss.item()=0.004344588611274958\n",
      "[198]: loss.item()=0.004203215707093477\n",
      "[199]: loss.item()=0.004066381137818098\n",
      "[200]: loss.item()=0.003933973144739866\n",
      "[201]: loss.item()=0.003805873915553093\n",
      "[202]: loss.item()=0.003681875066831708\n",
      "[203]: loss.item()=0.0035619125701487064\n",
      "[204]: loss.item()=0.003445790149271488\n",
      "[205]: loss.item()=0.003333417000249028\n",
      "[206]: loss.item()=0.0032246694900095463\n",
      "[207]: loss.item()=0.0031194568146020174\n",
      "[208]: loss.item()=0.003017549403011799\n",
      "[209]: loss.item()=0.0029190308414399624\n",
      "[210]: loss.item()=0.002823611255735159\n",
      "[211]: loss.item()=0.0027312221936881542\n",
      "[212]: loss.item()=0.002641848521307111\n",
      "[213]: loss.item()=0.00255529279820621\n",
      "[214]: loss.item()=0.0024715489707887173\n",
      "[215]: loss.item()=0.002390508772805333\n",
      "[216]: loss.item()=0.0023120271507650614\n",
      "[217]: loss.item()=0.002236059634014964\n",
      "[218]: loss.item()=0.0021625496447086334\n",
      "[219]: loss.item()=0.0020914003252983093\n",
      "[220]: loss.item()=0.002022494561970234\n",
      "[221]: loss.item()=0.0019558349158614874\n",
      "[222]: loss.item()=0.0018912508385255933\n",
      "[223]: loss.item()=0.0018287885468453169\n",
      "[224]: loss.item()=0.0017683333717286587\n",
      "[225]: loss.item()=0.0017097695963457227\n",
      "[226]: loss.item()=0.001653097104281187\n",
      "[227]: loss.item()=0.001598266651853919\n",
      "[228]: loss.item()=0.0015451759099960327\n",
      "[229]: loss.item()=0.0014938024105504155\n",
      "[230]: loss.item()=0.0014440505765378475\n",
      "[231]: loss.item()=0.0013959065545350313\n",
      "[232]: loss.item()=0.0013493139995262027\n",
      "[233]: loss.item()=0.0013042035279795527\n",
      "[234]: loss.item()=0.001260561402887106\n",
      "[235]: loss.item()=0.0012183318613097072\n",
      "[236]: loss.item()=0.0011774408631026745\n",
      "[237]: loss.item()=0.0011378709459677339\n",
      "[238]: loss.item()=0.0010995989432558417\n",
      "[239]: loss.item()=0.0010625544236972928\n",
      "[240]: loss.item()=0.0010266881436109543\n",
      "[241]: loss.item()=0.0009920030133798718\n",
      "[242]: loss.item()=0.0009584282524883747\n",
      "[243]: loss.item()=0.0009259542566724122\n",
      "[244]: loss.item()=0.0008945215959101915\n",
      "[245]: loss.item()=0.0008641210733912885\n",
      "[246]: loss.item()=0.0008346724789589643\n",
      "[247]: loss.item()=0.0008062341366894543\n",
      "[248]: loss.item()=0.0007786955684423447\n",
      "[249]: loss.item()=0.0007520411745645106\n",
      "[250]: loss.item()=0.000726293888874352\n",
      "[251]: loss.item()=0.0007013540016487241\n",
      "[252]: loss.item()=0.0006772538181394339\n",
      "[253]: loss.item()=0.0006539340829476714\n",
      "[254]: loss.item()=0.0006313807680271566\n",
      "[255]: loss.item()=0.0006095702992752194\n",
      "[256]: loss.item()=0.000588487833738327\n",
      "[257]: loss.item()=0.0005680893082171679\n",
      "[258]: loss.item()=0.0005483570857904851\n",
      "[259]: loss.item()=0.0005292875575833023\n",
      "[260]: loss.item()=0.0005108527839183807\n",
      "[261]: loss.item()=0.0004930205177515745\n",
      "[262]: loss.item()=0.00047578246449120343\n",
      "[263]: loss.item()=0.00045911071356385946\n",
      "[264]: loss.item()=0.0004430163244251162\n",
      "[265]: loss.item()=0.0004274383536539972\n",
      "[266]: loss.item()=0.00041238300036638975\n",
      "[267]: loss.item()=0.0003978545719292015\n",
      "[268]: loss.item()=0.00038377899909392\n",
      "[269]: loss.item()=0.0003702057874761522\n",
      "[270]: loss.item()=0.00035707768984138966\n",
      "[271]: loss.item()=0.00034437834983691573\n",
      "[272]: loss.item()=0.0003321319818496704\n",
      "[273]: loss.item()=0.00032027895213104784\n",
      "[274]: loss.item()=0.00030883794534020126\n",
      "[275]: loss.item()=0.000297791906632483\n",
      "[276]: loss.item()=0.00028712276252917945\n",
      "[277]: loss.item()=0.0002767972182482481\n",
      "[278]: loss.item()=0.000266835413640365\n",
      "[279]: loss.item()=0.00025721362908370793\n",
      "[280]: loss.item()=0.0002479183895047754\n",
      "[281]: loss.item()=0.00023894397600088269\n",
      "[282]: loss.item()=0.0002302828070241958\n",
      "[283]: loss.item()=0.00022191049356479198\n",
      "[284]: loss.item()=0.00021383707644417882\n",
      "[285]: loss.item()=0.00020603847224265337\n",
      "[286]: loss.item()=0.00019850439275614917\n",
      "[287]: loss.item()=0.00019124987011309713\n",
      "[288]: loss.item()=0.00018422622815705836\n",
      "[289]: loss.item()=0.00017746227968018502\n",
      "[290]: loss.item()=0.0001709212810965255\n",
      "[291]: loss.item()=0.00016462543862871826\n",
      "[292]: loss.item()=0.00015852735668886453\n",
      "[293]: loss.item()=0.00015266405534930527\n",
      "[294]: loss.item()=0.0001469987037125975\n",
      "[295]: loss.item()=0.00014153918891679496\n",
      "[296]: loss.item()=0.00013626200961880386\n",
      "[297]: loss.item()=0.00013117508206050843\n",
      "[298]: loss.item()=0.00012626837997231632\n",
      "[299]: loss.item()=0.00012153845455031842\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lossf = nn.MSELoss()\n",
    "\n",
    "# for _ in tqdm(range(EPOCHS)):\n",
    "for idx in range(EPOCHS):\n",
    "    opt.zero_grad()\n",
    "    loss = lossf(model(Xs).rename(None).unsqueeze(-1), ys)\n",
    "    print(f\"[{idx}]: {loss.item()=}\")\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0195,  0.0080, -0.0049,  0.9991], grad_fn=<SqueezeBackward1>,\n",
       "       names=('batch',))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
